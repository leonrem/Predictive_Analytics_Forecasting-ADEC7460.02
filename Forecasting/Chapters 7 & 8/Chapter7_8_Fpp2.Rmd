---
title: "FPP2 Chapters 7 and 8"
author: Leonid Rempel
output: 
  html_document:
    fig_width: 10
    code_folding: hide
    theme: readable
    highlight: pygments
    toc: yes
    toc_depth: 3
    toc_float: true
    fig_caption: yes
---

```{r include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r message=FALSE}
library(printr)
library(fpp2)
library(tidyverse)
library(ggthemes)
library(jtools)

theme_set(theme_clean())
```

# Chapter 7 Exercises
## Question 1

### a)

Here are the optimal paramaters R comes up with.

```{r}
pigs.ses <- ses(pigs, h= 4)
pigs.ses$model
```

Here is are the forecasts for the next 4 months.

```{r results = 'asis'}
pigs.ses %>% 
  kableExtra::kbl(caption = 'Pigs SES Forecasts') %>% 
  kableExtra::kable_styling(full_width = F)
```

And it is also presented as a plot.

```{r}
autoplot(pigs.ses)
```


### b)

Below are my lower and upper intervals 

```{r}
s <- sd(pigs.ses$residuals)
upper <- pigs.ses$mean[1] + 1.96*s
lower <- pigs.ses$mean[1] - 1.96*s

paste(lower, upper, sep = ',')

```

As we can see, my 95% interval is very close to the one produced by the ses.

## Question 2 

```{r echo=TRUE, message=FALSE, warning=FALSE, Question2}

SES <- function(y, alpha, level){
  y_hat <- level
  for(index in 1:length(y)){
   y_hat <- alpha*y[index] + (1 - alpha)*y_hat 
  }
  return(y_hat)
}

alpha <- pigs.ses$model$par[1]
level <- pigs.ses$model$par[2]
fc <- SES(pigs, alpha = alpha, level = level)

paste('The next forecast using my function is: ', fc)

```

This forecast is identical to the `ses()` forecast of 98816.41. 

## Question 3

```{r echo=TRUE, message=FALSE, warning=FALSE, Question3}
# modify SES function to return SSE
SES <- function(params = c(alpha, level), y) {
  error <- 0
  SSE <- 0
  alpha <- params[1]
  level <- params[2]
  y_hat <- level
  
  for(index in 1:length(y)){
    error <- y[index] - y_hat
    SSE <- SSE + error^2
    
    y_hat <- alpha*y[index] + (1 - alpha)*y_hat 
  }
  
  return(SSE)
}


opt_SES_pigs <- optim(par = c(0.5, pigs[1]), y = pigs, fn = SES)

paste('The parameters produced are : ', opt_SES_pigs$par[1], opt_SES_pigs$par[2])


```

Here we have that the alpha is pretty much the same although the initial level is different.

## Question 4

```{r echo=TRUE, message=FALSE, warning=FALSE, Question4}

SES <- function(init_params, data){

  fc_next <- 0
  
  SSE <- function(params, data){
    error <- 0
    SSE <- 0
    alpha <- params[1]
    level <- params[2]
    y_hat <- level
    
    for(index in 1:length(data)){
      error <- data[index] - y_hat
      SSE <- SSE + error^2
      
      y_hat <- alpha*data[index] + (1 - alpha)*y_hat 
    }
    fc_next <<- y_hat
    return(SSE)
  }
  
  optim_pars <- optim(par = init_params, data = data, fn = SSE)
  
  return(list(
    next_fc = fc_next,
    alpha = optim_pars$par[1],
    level = optim_pars$par[2]
    ))
}

mycalc <- SES(c(0.5, pigs[1]), pigs)

paste("Next observation forecast by ses function is: ", pigs.ses$mean[1], 'and my function returns ', mycalc$next_fc)

paste("The alpha calculated by ses function is: ", pigs.ses$model$par[1], 'and my function returns', mycalc$alpha)

paste("The level calculated from the ses function is ", pigs.ses$model$par[2], 'and my function returns', mycalc$level)

```

## Question 5

### a)

```{r}
autoplot(books)
```

We have an upward trend with weekly seasonality for daily books sales.

### b)

```{r}
paper.ses <- ses(books[,"Paperback"], h=4)
autoplot(paper.ses)
```

```{r}
hard.ses <- ses(books[,"Hardcover"], h=4)
autoplot(hard.ses)
```

### c)

```{r}
paste('The RMSE of the paperback books is: ', sqrt(mean(paper.ses$residuals^2)))

paste('The RMSE of the hardcover books is: ', sqrt(mean(hard.ses$residuals^2)))
```

## Question 6

### a)

```{r}
paper.holt <- holt(books[,"Paperback"], h=4)
hard.holt <- holt(books[,"Hardcover"], h=4)
```

### b)

```{r}
paste('The RMSE of the paperback books holt model is: ', sqrt(mean(paper.holt$residuals^2)))

paste('The RMSE of the hardcover books holt is: ', sqrt(mean(hard.holt$residuals^2)))
```

### c)

First we compare the paperback sales.

```{r}
par(mfrow = c(2,1))
autoplot(paper.holt)
autoplot(paper.ses)
```

Then the hardcover sales. 

```{r}
par(mfrow = c(2,1))
autoplot(hard.holt)
autoplot(hard.ses)
```

The Holt model performs better in both instances.

### d)

```{r}
paper.holt.sd <- sd(paper.holt$residuals)
paper.holt.upper <- paper.holt$mean[1] + 1.96*paper.holt.sd
paper.holt.lower <- paper.holt$mean[1] - 1.96*paper.holt.sd

paste("The holt interval for the paperback is: ", paste(paper.holt.lower, paper.holt.upper, sep = ', '))

paper.ses.sd <- sd(paper.ses$residuals)
paper.ses.upper <- paper.ses$mean[1] + 1.96*paper.ses.sd
paper.ses.lower <- paper.ses$mean[1] - 1.96*paper.ses.sd

paste("The ses interval for the paperback is: ", paste(paper.ses.lower, paper.ses.upper, sep = ', '))

hard.holt.sd <- sd(hard.holt$residuals)
hard.holt.upper <- hard.holt$mean[1] + 1.96*hard.holt.sd
hard.holt.lower <- hard.holt$mean[1] - 1.96*hard.holt.sd

paste("The holt interval for the hardback is: ", paste(hard.holt.lower, hard.holt.upper, sep = ', '))

hard.ses.sd <- sd(hard.ses$residuals)
hard.ses.upper <- hard.ses$mean[1] + 1.96*hard.ses.sd
hard.ses.lower <- hard.ses$mean[1] - 1.96*hard.ses.sd

paste("The ses interval for the hardback is: ", paste(hard.ses.lower, hard.ses.upper, sep = ', '))

```

The `holt()` intervals are upwardly biased vis-a-vis the `ses()` model confidence interval.

## Question 7

Lets take a look at the eggs dataset.

```{r}
autoplot(eggs) + 
  labs(title = 'Price of a dozen eggs in the United States',
       y = 'Price',
       x = 'Year')

eggs.train <- eggs[1:90]
eggs.test <- eggs[90:94]
```

I am going to use 4 years as a holdout set. First we look at the forecasts with the plain `holt()` model.

```{r}
autoplot(holt(eggs, h = 100))
accuracy(holt(eggs.train, h = 100), x = eggs.test) %>% 
  kableExtra::kbl(caption = 'Holt Performance') %>% 
  kableExtra::kable_styling(full_width = F)
```

Next use the `holt()` dampened model.

```{r}
autoplot(holt(eggs, h = 100, damped = T))
accuracy(holt(eggs.train, h = 100, damped = T), x = eggs.test) %>% 
  kableExtra::kbl(caption = 'Holt Dampened Performance') %>% 
  kableExtra::kable_styling(full_width = F)
```

Lastly, lets try a `holt()` model with `exponential = TRUE`.

```{r}
autoplot(holt(eggs, h = 100, damped = T, exponential = T))
accuracy(holt(eggs.train, h = 100, damped = T, exponential = T), x = eggs.test) %>% 
  kableExtra::kbl(caption = 'Holt Dampened and Exponential Performance') %>% 
  kableExtra::kable_styling(full_width = F)
```

On the training set, the regular`holt()` model performs best when looking at the RMSE and on the test set.



## Question 10

### a)
```{r}
autoplot(ukcars)
```

We can see a seasonal and an upward trend component after 1983.

### b) 

```{r}
stlcars = stl(ukcars, s.window=4, robust=TRUE)
autoplot(stlcars)
```

```{r}
seas.stlcars = seasadj(stlcars)
autoplot(seas.stlcars)
```

We can see that the seasonally adjusted graph is much smoother.

### c)

```{r}
add.damp.stl = stlf(seas.stlcars, etsmodel= "AAN", damped=TRUE, h=8)
autoplot(add.damp.stl)
```

### d)
```{r}
holtfc = stlf(seas.stlcars, etsmodel="AAN", damped=FALSE, h=8)
autoplot(holtfc)
```

### e)

```{r}
cars.ets = ets(ukcars)
summary(cars.ets)
```

The `ets()` chose an additive exponential and seasonal component.

### f)

```{r}

accuracy(add.damp.stl)%>% 
  kableExtra::kbl(caption = 'Damp Stl Performance') %>% 
  kableExtra::kable_styling(full_width = F)

accuracy(holtfc)%>% 
  kableExtra::kbl(caption = 'Holt Performance') %>% 
  kableExtra::kable_styling(full_width = F)

accuracy(cars.ets)%>% 
  kableExtra::kbl(caption = 'ETS Performance') %>% 
  kableExtra::kable_styling(full_width = F)
```

We can see that the best in sample fits come from the dampened stl model.

### g)
``` {r}
autoplot(add.damp.stl)
```

``` {r}
autoplot(holtfc)
```

``` {r}
autoplot(forecast(cars.ets, h = 8))
```

I would think that the ETS has the most reasonable forecast.

### h)
```{r}
checkresiduals(cars.ets)
```

The residuals could be better but they don't look too bad.

## Question 11

### a) 
```{r}
autoplot(visitors,
     main="Overseas visitors to Australia",
     ylab="Thousands of people",
     xlab="Year")
```

We can see a monthly seasonal component coupled with what looks like an additive upward trend.

### b) 
```{r}
visit.test = window(visitors, start = c(2003,4))
visit.train = window(visitors, end = c(2003,3))
```

Lets fit the model on the training set using `hw()`, then forecast the test set with the test set on the same plot.

```{r}
holt.visit <- hw(visit.train, h = 24, 
                 seasonal = 'multiplicative')
autoplot(visit.test) + 
  autolayer(holt.visit, PI = F, series = 'Holt-Winters Forecast') +
  labs(title = 'Test Set vs Holt Winters Multiplicative Forecast',
       y = 'Thousands of People',
       x = 'Year')
```

We can see that the forecasts miss the mark the longer we move in the forecast.

### c)

We need a multiplicative seasonal component in our model because the seasonal variation changes the farther out the series gets.

### d) 

`ets()` model forecasts are shown below.

```{r}
visit.ets.fc <- forecast(ets(visit.train), h = 24)
autoplot(visit.test) + 
  autolayer(holt.visit, PI = F, series = 'Holt-Winters Forecast') +
  autolayer(visit.ets.fc, PI = F, series = 'ETS Forecast') +
  labs(title = 'Test Set vs Holt and ETS',
       y = 'Thousands of People',
       x = 'Year')
```

We can see that the results form the ETS are very similar to the Holt-Winters, this is because the ETS selects the Holt-Winters as the model of best fit.

Lets use a box-cox transformation first.
```{r}
visit.ets.bx.fc <- forecast(ets(visit.train, lambda = BoxCox.lambda(visit.train),
                                additive.only = T), h = 24)
autoplot(visit.test) + 
  autolayer(holt.visit, PI = F, series = 'Holt-Winters Forecast') +
  autolayer(visit.ets.bx.fc, PI = F, series = 'Box-Cox ETS Forecast') +
  labs(title = 'Test Set vs Holt and Transformed ETS',
       y = 'Thousands of People',
       x = 'Year')
```

We have the same outcome.

Lets look at the seasonal naive model.

```{r}
visit.naive <- snaive(visit.train, h = 24)
autoplot(visit.test) + 
  autolayer(holt.visit, PI = F, series = 'Holt-Winters Forecast') +
  autolayer(visit.naive, PI = F, series = 'Naive Seasonal Forecast') +
  labs(title = 'Test Set vs Holt and Naive Seasonal ETS',
       y = 'Thousands of People',
       x = 'Year')
```

The naive seasonal forecast seems to do better that the Holt-Winters or our previous ETS models.

Lets take a look at an ETS with an STL decomposition. 
```{r}
visit.ets.bx.stl.ets <- visit.train %>% stlm(lambda = BoxCox.lambda(visit.train), s.window = 13,
                                       robust=TRUE, method="ets") %>% forecast(h=24)
autoplot(visit.test) + 
  autolayer(holt.visit, PI = F, series = 'Holt-Winters Forecast') +
  autolayer(visit.ets.bx.stl.ets, PI = F, series = 'STL ETS Forecast') +
  labs(title = 'Test Set vs Holt and STL ETS Forecast',
       y = 'Thousands of People',
       x = 'Year')
```

It seems as though the STL ETS forecast has the best results.
```{r}
cbind(c('Holt', 'ETS', 'ETS BC', 'ETS STL'), rbind(accuracy(holt.visit, x = visit.test)[2,], rbind(accuracy(visit.ets.fc, x = visit.test)[2,]),
      accuracy(visit.ets.bx.fc, x = visit.test)[2,], accuracy(visit.ets.bx.stl.ets, x = visit.test)[2,])) %>% kableExtra::kbl(caption = 'Model Performance Comparison') %>% 
  kableExtra::kable_styling(full_width = F)
```

If we go by MASE, the last model, the ETS STL model is the best fit, doing better (as opposes to worse) than Naive forecasts.

### e)

```{r}
checkresiduals(visit.ets.bx.stl.ets)
```

We can see good residual results for the STL ETS model.

### f)
```{r}

fets_add_BoxCox <- function(y, h) {
  forecast(ets(
    y,
    lambda = BoxCox.lambda(y),
    additive.only = TRUE
  ),
  h = h)
}

fstlm <- function(y, h) {
  forecast(stlm(
    y, 
    lambda = BoxCox.lambda(y),
    s.window = frequency(y) + 1,
    robust = TRUE,
    method = "ets"
  ),
  h = h)
}

fets <- function(y, h) {
  forecast(ets(y),
           h = h)
  }

# Lets use RMSE for easier comparison
paste('Holt-Winters RMSE: ', sqrt(mean(tsCV(visitors, hw, h = 24, 
               seasonal = "multiplicative")^2,
          na.rm = TRUE)))

paste('ETS RMSE: ', sqrt(mean(tsCV(visitors, fets, h = 24)^2, na.rm = TRUE)))

paste('ETS BoxCox RMSE: ', sqrt(mean(tsCV(visitors, fets_add_BoxCox, h = 24)^2,
          na.rm = TRUE)))

paste('Seasonal Naive RMSE: ', sqrt(mean(tsCV(visitors, snaive, h = 24)^2, na.rm = TRUE)))

paste('ETS STL RMSE: ', sqrt(mean(tsCV(visitors, fstlm, h = 24)^2,
          na.rm = TRUE)))
```
We can see from the `tsCV()` results that the ETS BoxCox transformation RMSE is lower than the ETS STL RMSE, but not by too much.

## Question 12

### a and b) 

Below we have the `tsCV()` results.

```{r}
paste('Seasonl Naive RMSE: ', sqrt(mean(tsCV(qcement, snaive, h = 4)^2, na.rm = T)))
paste('ETS RMSE: ', sqrt(mean(tsCV(qcement, fets, h = 4)^2, na.rm = T)))
```

We can see that the ETS performs better.



# Chapter 8 Exercises
## Question 1

### a)

For all three ACF graphs, we have that the autocorrelation is insignificant. This does in fact mean that we are looking at white noise - the random numbers that are produced are not related to one another over time.

### b)

The space of the critical values away from the residuals occrus due to the sample size, which narrows down the confidence interval the larger it is.





## Question 2

```{r}
tsdisplay(ibmclose)
```

We can see that the closing price of IBM that every lag of the ACF is significant for autocorrelation. On the PACF, however, it is only the first value so we should at least use the first difference when forecasting the stock price but it is not stationary - it will move over time.

## Question 3

For each of the below, we can get the appropriate number of differences by using `ndiffs()`

### a) 
```{r}
ndiffs(usnetelec)
```

### b)
```{r}
ndiffs(usgdp)
```

### c)
```{r}
ndiffs(mcopper)
```

### d)
```{r}
ndiffs(enplanements)
```

### e)
```{r}
ndiffs(visitors)
```




## Question 6
### a)
```{r}
y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100){
   y[i] <- 0.6*y[i-1] + e[i]
}
```

### b)
```{r}
ar1 <- function(phi){
  y <- ts(numeric(100))
  e <- rnorm(100)
  for(i in 2:100){
    y[i] <- phi*y[i-1] + e[i]
  }
  return(y)
}

```

Below we plot the different generated graphs.

```{r}
autoplot(ar1(0.3), series = "0.3") +
  geom_line(size = 1, colour = "red") +
  autolayer(y, series = "0.6", size = 1) +
  autolayer(ar1(0.9), size = 1, series = "0.9") +
  ylab("AR(1) models") +
  guides(colour = guide_legend(title = "Phi"))
```

### c)
```{r}
ma1 <- function(theta1){
  y <- ts(numeric(100))
  e <- rnorm(100)
  for(i in 2:100){
    y[i] <- theta1*e[i-1] + e[i]
  }
  return(y)
}
```

### d)
```{r}
autoplot(ma1(0.3), series = "0.3") +
  geom_line(size = 1, colour = "red") +
  autolayer(y, series = "0.6", size = 1) +
  autolayer(ma1(0.9), size = 1, series = "0.9") +
  ylab("MA(1) models") +
  guides(colour = guide_legend(title = "Theta"))
```

### e)
```{r}
y_arima.1.0.1 <- ts(numeric(50))
e <- rnorm(50)
for(i in 2:50){
   y_arima.1.0.1[i] <- 0.6*y_arima.1.0.1[i-1] + 0.6*e[i-1] + e[i]
}
```

### f)
```{r}
y_arima.2.0.0 <- ts(numeric(50))
e <- rnorm(50)
for(i in 3:50){
   y_arima.2.0.0[i] <- -0.8*y_arima.2.0.0[i-1] + 0.3*y_arima.2.0.0[i-2] + e[i]
}
```

### g)
```{r}
autoplot(y_arima.1.0.1, series = "ARMA(1, 1)") +
  autolayer(y_arima.2.0.0, series = "AR(2)") +
  ylab("y") +
  guides(colour = guide_legend(title = "Models"))
autoplot(y_arima.1.0.1)
```

The AR(2) model increased with oscillation so it is non-stationary unlike the ARMA(1, 1) which is stationary.

## Question 7

### a)
```{r}
autoplot(wmurders)
```

```{r}
autoplot(diff(wmurders))
```

It seems like one differencing should make the data stationary.

```{r}
ndiffs(wmurders)
```

The `ndiffs()` function says that we need 2 differencing.

```{r}
autoplot(diff(wmurders, differences = 2))
```

We get stationary data after differencing twice.

### b)

I wouldn't include a constant in the model since it will be integrated twice and generate a quadratic trend which can swing the forecasts one way or the other strongly.

### c)

$$ (1 - B)^2*yt = (1 + theta_1*B + theta_2*B^2)*et $$
### d)
```{r}
wmurders_arima.0.2.2 <- Arima(wmurders, 
                              order = c(0, 2, 2))
checkresiduals(wmurders_arima.0.2.2)
```

The residuals don't look too bad, but they could be more normal.

### e)

```{r}
fc_wmurders_arima.0.2.2 <- forecast(
  wmurders_arima.0.2.2, h = 3
)

fc_wmurders_arima.0.2.2$mean

fc_wmurders_arima.0.2.2$model
```

To calculate by hand we plug back into our formula.
```{r}
years <- length(wmurders)
e <- fc_wmurders_arima.0.2.2$residuals
fc1 <- 2*wmurders[years] - wmurders[years - 1] - 1.0181*e[years] + 0.1470*e[years - 1]
fc2 <- 2*fc1 - wmurders[years] + 0.1470*e[years]
fc3 <- 2*fc2 - fc1
```

Here are our forecasts:
```{r}
c(fc1, fc2, fc3)
```

They are similar to the forecasts from the actual function.

### f)
```{r}
autoplot(fc_wmurders_arima.0.2.2)
```

### g)
```{r}
fc_wmurders_autoarima <- forecast(
  auto.arima(wmurders), h = 3
)

accuracy(fc_wmurders_arima.0.2.2)
accuracy(fc_wmurders_autoarima)
```

We can see that the Arima model that we came up with does better on the training set when comparing by the MASE. But this difference is not significant.

## Question 8
### a)

```{r}
autoplot(austa)
fc_austa_autoarima <- forecast(
  auto.arima(austa), h = 10
)
fc_austa_autoarima$model
```

We can see that the ARIMA(0,1,1) with drift was selected by the auto.arima() 

```{r}
checkresiduals(fc_austa_autoarima)
```

The residuals look like white noise, so its pretty good. Lets look at the forecast.

```{r}
autoplot(fc_austa_autoarima)
```

### b)

```{r}
fc_austa_arima.0.1.1 <- forecast(
  Arima(austa, order = c(0, 1, 1)), h = 10
)
autoplot(fc_austa_arima.0.1.1)
fc_austa_arima.0.1.0 <- forecast(
  Arima(austa, order = c(0, 1, 0)), h = 10
)
autoplot(fc_austa_arima.0.1.0)
```

We can see that without the drift the forecasts are more stationary but the confidence interval is narrower with the MA term. These forecasts are no better than the naive model.

### c)
```{r}
fc_austa_arima.2.1.3.drift <- forecast(
  Arima(austa, order = c(2, 1, 3), include.drift = TRUE),
  h = 6
)
autoplot(fc_austa_arima.2.1.3.drift)
```

We have something of a dampened outcome with the drift.

### d)
```{r}
fc_austa_arima.0.0.1.const <- forecast(
  Arima(
    austa, order = c(0, 0, 1), include.constant = TRUE
    ),
  h = 10
)
autoplot(fc_austa_arima.0.0.1.const)

fc_austa_arima.0.0.0.const <- forecast(
  Arima(austa, order = c(0, 0, 0), include.constant = TRUE),
  h = 10
)
autoplot(fc_austa_arima.0.0.0.const)
```

The new forecasts have a sharp decrease followed by a naive trend.

### e)
```{r}
fc_austa_arima.0.2.1 <- forecast(
  Arima(austa, order = c(0, 2, 1)),
  h = 10
)
autoplot(fc_austa_arima.0.2.1)
```

We have an increasing trend with an increasing confidence interval.




## Question 9
### a)

```{r}
autoplot(usgdp)
autoplot(BoxCox(usgdp, BoxCox.lambda(usgdp)))
```

The BoxCox transformation helps make the timeseries more linear, so it should be used in the forecasting.

### b)
```{r}
lambda_usgdp <- BoxCox.lambda(usgdp)
usgdp_autoarima <- auto.arima(usgdp, 
                              lambda = lambda_usgdp)
autoplot(usgdp, series = "Data") +
  autolayer(usgdp_autoarima$fitted, series = "Fitted")
usgdp_autoarima
```

We can see a really good fit on the data with the ARIMA(2,1,0) with drift model.

### c)
```{r}
ndiffs(BoxCox(usgdp, lambda_usgdp))
ggtsdisplay(diff(BoxCox(usgdp, lambda_usgdp)))
```

We need to difference once for stationarity.

We see a spike at lag 12, but our data is quarterly so this can be ignored.

```{r}
usgdp_arima.1.1.0 <- Arima(
  usgdp, lambda = lambda_usgdp, order = c(1, 1, 0)
)
usgdp_arima.1.1.0
autoplot(usgdp, series = "Data") +
  autolayer(usgdp_arima.1.1.0$fitted, series = "Fitted")
```

Lets also try an ARIMA with drift
```{r}
usgdp_arima.1.1.0.drift <- Arima(
  usgdp, lambda = lambda_usgdp, order = c(1, 1, 0),
  include.drift = TRUE
)
usgdp_arima.1.1.0.drift
autoplot(usgdp, series = "Data") +
  autolayer(usgdp_arima.1.1.0.drift$fitted, series = "Fitted")
```

Everything seems like a really good fit.

### d)
```{r}
accuracy(usgdp_autoarima)
accuracy(usgdp_arima.1.1.0)
accuracy(usgdp_arima.1.1.0.drift)
```

We can see that the `auto.arima()` fit the data best.

### e)
```{r}
autoplot(forecast(auto.arima(usgdp), h = 10))
```

The Forecasts seem reasonable and our confidence interval is tight around the predictions.

### f)
```{r}
autoplot(forecast(ets(usgdp), h = 10))
```

The predictions look similar to the Auto Arima model.


## Question 10
### a)
```{r}
autoplot(austourists)
```

We have strong seasonality and an increasing trend. It seems like the seasonality is multiplicative. 

### b)
```{r}
ggAcf(austourists)
```

From the ACF we can see that the autocorrelations get weaker over time, furthermore, we have a large lag every 4 periods which makes sense given that we have quarterly data.

### c)
```{r}
ggPacf(austourists)
```

We can see only 5 significant lags and then the effect is not as important. We may need to difference out to 4 or 5 lags.

### d)
```{r}
ggtsdisplay(diff(austourists, lag = 4))
```

We can see that differencing helped although one more lag might be useful.

We need an arima model with seasonality, so something like a (1, 1, 0)(0, 1, 1) with [4] differencing.

### e)
```{r}
fc_austourists_autoarima <- forecast(
  auto.arima(austourists)
)
fc_austourists_autoarima$model
```

The `auto.arima()` gives an ARIMA(1, 0, 0)(1, 1, 0)[4] model.

```{r}
checkresiduals(fc_austourists_autoarima)
```

We have residuals that look like white noise.

## Question 11
### a)
```{r}
usmelec_ma2x12 <- ma(usmelec, order = 12, centre = TRUE)
autoplot(usmelec, series = "Data") +
  autolayer(usmelec_ma2x12, series = "2X12-MA") +
  ylab(expression(paste("Electricity(x", 10^{9}, "KWh)"))) + 
  ggtitle("Monthly total net generation of electricity") +
  scale_color_discrete(breaks = c("Data", "2X12-MA"))
```

### b)
We can see that the variation in data increases over time so a transformation is neccessary. 
```{r}
lambda_usmelec <- BoxCox.lambda(usmelec)
lambda_usmelec
```

### c)
The data is not stationary. 
```{r}
ndiffs(usmelec)
nsdiffs(usmelec)
```

We need to do one difference and one seasonal difference to make the data stationary.

### d)
```{r}
ggtsdisplay(diff(
  BoxCox(usmelec, lambda_usmelec),
  lag = 12
  ))
```

Lets add in the one differencing. 

```{r}
ggtsdisplay(
  diff(diff(BoxCox(usmelec, lambda_usmelec),lag = 12))
)
```
This leads to white noise residuals.

In this case I would try an ARIMA(0, 1, 2)(0, 1, 1)[12] with the BoxCox transformation.
```{r}
usmelec_arima.0.1.2.0.1.1.12 <- Arima(
  usmelec,
  lambda = lambda_usmelec,
  order = c(0, 1, 2),
  seasonal = c(0, 1, 1)
)
```

### e)
```{r}
checkresiduals(usmelec_arima.0.1.2.0.1.1.12)
```

We have mostly white noise for our model.



## Question 12
### a)
```{r}
autoplot(mcopper)
```

We can see some seasonality, and a sudden increase in the price of copper later in the 2000s.

Lets see
```{r}
autoplot(BoxCox(mcopper, BoxCox.lambda(mcopper)))
lambda_mcopper <- BoxCox.lambda(mcopper)
```

We can see that the boxplot does make the data more stationary, taking account for some of the sudden price increase so this added stability would be useful for forecasting.

### b)
```{r}
mcopper_autoarima <- auto.arima(mcopper,lambda = lambda_mcopper)
mcopper_autoarima
```

The `auto.arima()` gives an ARIMA(0,1,1) model with the lambda transform.

### c)
```{r}
ndiffs(mcopper)
nsdiffs(mcopper)
```

New models need to include 1 differencing.

```{r}
ggtsdisplay(diff(mcopper))
```

We can see a sinusouidal decrease in the autocorrelation values so we can choose (1, 1, 0) or, lookin at the PACF, a (5, 1, 0).

```{r}
mcopper_arima.1.1.0 <- Arima(mcopper, order = c(1, 1, 0), lambda = lambda_mcopper)
mcopper_arima.1.1.0
```

```{r}
mcopper_arima.5.1.0 <- Arima(mcopper, order = c(5, 1, 0), lambda = lambda_mcopper)
mcopper_arima.5.1.0
```

The AIC is lower with the (5,1,0) model.

```{r}
# AICc was -78.48.
# I'll try auto.arima function without approximation and stepwise options.
mcopper_autoarima2 <- auto.arima(
  mcopper, lambda = lambda_mcopper,
  approximation = FALSE, stepwise = FALSE
)
mcopper_autoarima2
```

### d)
I am going to proceed with the auto.arima boxcox transformed model. Lets check the residuals.
```{r}
checkresiduals(mcopper_autoarima)
```

We can see that the residuals are pretty good.

### e)
```{r}
autoplot(forecast(mcopper_autoarima))
```

The forecast doesn't look good, we can try some of the other models.
```{r}
autoplot(forecast(mcopper_arima.1.1.0))
```

The (1,1,0) model has the same outcome.

```{r}
autoplot(forecast(mcopper_arima.5.1.0))
```

The (5,1,0) produces about the same thing.

### f)
```{r}
autoplot(forecast(ets(mcopper)))
```

The `ets()` does a better job than the ARIMA models.

## Question 13
### a)
Lets look at hsales.
```{r}
autoplot(hsales)
```

This data looks pretty stationary, and so I will not be transforming

### b)
We can confirm whether or not the data is stationary by using the `ndiffs()` and the `nsdiffs()`.
```{r}
ndiffs(hsales)
nsdiffs(hsales)
```

As suspected, there is no need to difference, although there is a seasonal pattern.

### c)
```{r}
ggtsdisplay(hsales)
```

We can see that we should add the seasonal differencing. Lets see the `auto.armia()` give us the numbers.

```{r}
auto.arima(hsales)
```

We can see that there is also a drift component and the `auto.armia()` decided on 1st differencing. 

### d)
```{r}
checkresiduals(auto.arima(hsales))
```

We can see that the residuals could be better but they are not the worst.

### e)
```{r}
autoplot(forecast(auto.arima(hsales), h = 24))
```

The forecasts look reasonable, although they are very stationary (we dont see any shigt upwards or downwards).

### f)
```{r}
autoplot(forecast(ets(hsales), h = 24))
```

The `ets()` is very similar to the `auto.arima()` model.

## Question 14

```{r}
hsales_stlf <- stlf(hsales, s.window = 5, robust = TRUE, method = "arima", h = 24)
```

```{r}
autoplot(hsales_stlf)
```

The `stlf()` functions does quite a similar job, I would honestly think that it is better because the seasonal trend is very present in hsales.

## Question 16
### a)
```{r}
autoplot(sheep)
```

We have a decreasing trend without any clear seasonality.

### b)

The ARIMA model is (3,1,0).

### c)
```{r}
ggtsdisplay(diff(sheep))
```

We can see that this is appropriate because the significant spikes of the PACF go out to 3 lags.

### d)
Below are the forecasts for the next three years.

```{r}
sheep.1940 = 1797 + 0.42*(1797 - 1791) -0.20*(1791 - 1627) - 0.30*(1627 - 1665)
sheep.1941 = sheep.1940 + 0.42*(sheep.1940 - 1797) -0.20*(1797 - 1791) - 0.30*(1791 - 1627)
sheep.1942 = sheep.1941 + 0.42*(sheep.1941 - sheep.1940) -0.20*(sheep.1940 - 1797) - 0.30*(1797 - 1791)
c(sheep.1940, sheep.1941, sheep.1942)
```

### e)
```{r}
forecast(Arima(sheep, order = c(3, 1, 0)),h = 3)$mean
```

We have basically the same foretasted values, the differences are likely due to rounding errors.
## Question 17
### a)
```{r}
autoplot(bicoal)
```

### b)

This is an ARIMA(4, 0, 0) or AR(4) model.

### c)
```{r}
ggAcf(bicoal, lag.max = 36)
ggPacf(bicoal, lag.max = 36)
```

ACF plot shows sinusoidally decreasing autocorrelation values while the PACF plot shows significant spikes at lag 1 and 4, but none beyond lag 4 which is why the AR(4) model was chosen.

### d)
```{r}
c = 162.00
phi1 = 0.83 
phi2 = -0.34
phi3 = 0.55
phi4 = -0.38
bicoal.1969 <- c + phi1*545 + phi2*552 + phi3*534 + phi4*512
bicoal.1970 <- c + phi1*bicoal.1969 + phi2*545 + phi3*552 + phi4*534
bicoal.1971 <- c + phi1*bicoal.1970 + phi2*bicoal.1969 + phi3*545 + phi4*552
c(bicoal.1969, bicoal.1970, bicoal.1971)
```

### e)
```{r}
forecast(ar(bicoal, 4), h = 3)$mean
```
We have very similar forecasts, again the differences are due to rounding coefficients. 

## Question 18
### a)

```{r include = F}
library(Quandl)
Quandl.api_key("iy5uZUKknxBqxqBZveHb")
```

```{r}
y <- Quandl.datatable('ZILLOW/DATA', indicator_id='ZSFH', region_id='99999')

y <- y %>%
  arrange(date)

# We just want the values
y <- ts(y$value, start = c(2005, 01), frequency = 12)
```

We are going to use the ZHVI Single-Family Homes Time Series ($)

### b)
```{r}
autoplot(y)
```

We can see an upward trend without seasonality. Lets look at the breakdown by ACF and PACF

```{r}
ggtsdisplay(y)
```

We can see that there is strong autocorrelation (which is expected) in addition to one large 1 lag spike for the PACF.

Lets see what `ndiffs()` suggests.
```{r}
ndiffs(y)
```

Because `ndiffs()` suggests 2 differencing, I would suggest an ARIMA(0, 2, 2)(0, 0, 1) model. 

```{r}
y.arima <- auto.arima(y)
y.arima
```

R agrees with the analysis and we have a linear exponential smoothing model.

### c)
```{r}
checkresiduals(y.arima)
```


Besides the anomaly that is 2020, we have white noise. 

### d)
```{r}
autoplot(forecast(y.arima, h = 24))
```

It is interesting that the ARIMA forecasts a spike in the future - maybe this will absorb the trend decrease from the lead up to 2020? 

### e)

Lets have R fit and `ets()` model.
```{r}
ets(y)
```

We have an `ets()` with a moving average and an additive trend. As stated before, we do not have seasonality so the seasonality component is null.

### f)
Lets check the residuals of the R chosen `etf()` model.
```{r}
checkresiduals(ets(y))
```

We can see that the `ets()` handles the data pretty well and separates out a lot of the white noise.

### g)
```{r}
autoplot(forecast(ets(y), h=24))
```

The `ets()` model does not produce a small bump which is interesting but the confidence interval varies much wider into the future.

### h)
I would prefer the ARIMA model because of the narrower confidence intervals.


