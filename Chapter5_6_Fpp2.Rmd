---
title: "FPP2 Chapters 5 and 6"
author: Leonid Rempel
output: 
  html_document:
    fig_width: 10
    code_folding: hide
    theme: readable
    highlight: pygments
    toc: yes
    toc_depth: 3
    toc_float: true
    fig_caption: yes
---

```{r include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

# Chapter 5 Exercises

First lets install the appropriate packages

```{r message=FALSE}
library(printr)
library(fpp2)
library(tidyverse)
library(ggthemes)
library(jtools)

theme_set(theme_clean())
```

## 5.1: Daily Electricity Demand
### a) Data exploration
```{r, fig.cap='Total electricity demand in GW for Victoria, Australia, every half-hour during 2014', fig.width=10}

# First we download the data from fpp2 to save as a dataframe with Date for pleasant aesthetics
data("elecdaily")
elec.df <- as.data.frame(elecdaily)
elec.df$date <- seq(from = as.Date("2014-01-01"), to = as.Date("2014-12-31"), by = 1)

elec.df%>% 
ggplot(aes(x = date, y = Demand)) + geom_line(color = 'red') +
  labs(title = 'Victoria, Australia Electricity usage during 2014',
       x = 'Date',
       y = 'Demand')

```

```{r, fig.cap='Half-hourly temeperatures for Melbourne in celsius', fig.width=10}

elec.df %>% 
ggplot(aes(x = date, y = Temperature)) + geom_line(color = 'blue') +
  labs(title = 'Victoria, Australia Temperature during 2014',
       x = 'Date',
       y = 'Temperature')

```

By the way, the temperature plot is accurate and starts in January. Melbourne experienced [maximum temperatures](http://www.bom.gov.au/climate/current/annual/vic/archive/2014.melbourne.shtml) on January 16-17, 2014. At first I thought the start date for the temperature collection was not January but it turns out it is! Lets go on to check out the relationship between daily electricity usage and temperature. 

```{r}
# lets use the fpp2 tslm() function
lm1 <- tslm(Demand ~ Temperature, data = elecdaily)
summ1 <- summ(lm1, model.info=T)
export_summs(summ1, output='html', 
             statistics = c("N" = "nobs", 
                            "R squared" = "r.squared", 
                            "P value" = "p.value",
                            "AIC" = "AIC"),
             model.names = c('Electricity Demand'))
```

We can see that there is a positive relationship between temperature and electricity demand. This makes sense - if the temperature increases, we would imagine people would cool themselves down by turning on a fan or air conditioning. Therefore, higher temperatures would lead to more electricity use as opposed to lower temperatures which would increase heat usage. For a quick reference, the average temperature in 2014 in Melbourne was `r mean(elecdaily[,3])` celsius. 

The question asks us to use the first 20 days, so from here on out we will be using the first 20 days data. The regression for it is given below. 

```{r}

daily20 <- head(elecdaily, 20)
daily.df <- as.data.frame(daily20)
daily.df$date <- seq(from = as.Date("2014-01-01"), to = as.Date("2014-01-20"), by = 1)

lm20 <- tslm(Demand ~ Temperature, data = daily20)
summ20 <- summ(lm20, model.info=T)
export_summs(summ20, output='html', 
             statistics = c("N" = "nobs", 
                            "R squared" = "r.squared", 
                            "P value" = "p.value",
                            "AIC" = "AIC"),
             model.names = c('Electricity Demand'))
```

We get the same positive relationship, but it is now significant at the 1% level.

### b) Checking our simple regression's assumptions

We can check our assumptions with checkresiduals() from the fpp2 package. 
```{r}
par(mfrow = c(2,2))
checkresiduals(lm20)
```

We can see that for the model which includes the first 20 days there are not outliers. Furthermore, we do not have evidence of auto correlation. A fit of the regression is plotted below. **NOTE** This would not be the case if we were to use the entire yearly daily dataset.

```{r}
daily.df %>% 
  ggplot(aes(x = Temperature, y = Demand)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se= F)
```

I would say this model is adequate for forecasting Demand or explaining the relationship between Temperature and Demand.

### c, d) Simple forecasts with prediction interval

```{r}
forecast <- forecast(lm20, 
                     newdata = data.frame(Temperature = c(15,35)))

knitr::kable(forecast) %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", 
                            full_width = F, 
                            position = 'float_right')
```
The first row lists the forecast for 15 degrees celsius and then 35 degrees celsius. I would say that the prediction for 35 degrees celsius is fairly accurate because there is a recorded temperature within the first 20 days that is close to 35 celsius (We have a data point that is 34 degrees celsius). The forecast for 15 degrees might be slightly less so, since the closest temperature we have to is is 19.6 degrees celsius. 

### e) Entire dataset
The entire plot can be found under part [a) Data exploration] and as is suggested by the plot and discussion there, this relationship is not as strong or useful for prediction electricity usage for the entirety of the year. 

## 5.2: Olympic Winning Times
### a) Data exploration
```{r fig.cap='Winning times (in seconds) for the men’s 400 meters final in each Olympic Games'}
data("mens400")
autoplot(mens400) +
  labs(title = 'Winning Olympic Times over the Years',
       x = 'Year',
       y = 'Winning Time')
```

Two things stand out from the graph of the winning times. First, we have some missing values, this is because the Olympic Games did not take place during WW1 or WW2. Secondly, The winning time has been decreasing throughout the years, which means the athletes are getting better. However, this trend has been leveling off since the late 1990s.

### b) Simple regression
```{r}
# To calculate the average rate of decrease, we will make a dataframe object
mens.df <- as.data.frame(mens400)

# I just wanted to have a proper dependent variable name
mens.df$win <- mens.df$x

# We could use a years variable, but I think its better to use a trend variable for better interpretability in the regression results
mens.df$t <- seq(0, (2016-1896), by = 4)

lm_mens <- tslm(win ~ t, data = mens.df)
summ_mens <- summ(lm_mens, model.info=T)
export_summs(summ_mens, output='html', 
             statistics = c("N" = "nobs", 
                            "R squared" = "r.squared", 
                            "P value" = "p.value",
                            "AIC" = "AIC"),
             model.names = c('Win Time'),
             coefs = c("Trend" = 't'))

# we can use the regular year variable for the actual plotting
autoplot(mens400) +
  labs(title = 'Winning Olympic Times over the Years',
       x = 'Year',
       y = 'Winning Time') + 
  geom_smooth(method = 'lm', se = F)
  
```

We can see that, on average, the winning time has decreased by around six seconds year over year. 

### c) Residuals of our simple regression
```{r}
checkresiduals(lm_mens)
```

Besides a couple outliers and a violation of auto correlation, we can say that the simple regression generally fits the data well. 

### d) Simple forecast
```{r}
forecast <- forecast(lm_mens, 
                     newdata = data.frame(t = 124))

knitr::kable(forecast) %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", 
                            full_width = F, 
                            position = 'float_right')
```

The point prediction for the win time of the men's 400 for the 2020 Olympics is 42 seconds, but it could confidently be as low as 39 seconds or as high as 44.5 seconds. This forecast, using a linear model, assumes the underlying times are given as a normal distribution but this isn't the case, as can be seen from [c) Residuals of out simple regression] 

## 5.3 Easter()
```{r include = F}
easter(ausbeer)
help("ausbeer")
```

Ausbeer returns total quarterly beer production in Australia (in megalitres) from 1956:Q1 to 2010:Q2.

Easter() returns a vector of 0's and 1's or fractional results if Easter spans March and April in the observed time period. Easter is defined as the days from Good Friday to Easter Sunday inclusively, plus optionally Easter Monday if easter.mon=TRUE.

## 5.4 Elasticity

$$
1:ln(y) = \beta_0 + \beta_1ln(x) + \epsilon \\
2: \beta_1*\frac{1}{x} = \frac{dy}{dx}*\frac{1}{y} \\
3:\beta_1 = \frac{dy}{dx}*\frac{x}{y}
$$

## 5.5 Shop Sales
### a) Data exploration
```{r}
autoplot(fancy) + 
  labs(title = 'Sales over time',
       y = 'Sales',
       x = 'Year')
```

We can see from the above graph that the sales are increasing over time and they are highly seasonal, as is expected from the problem description - sales spike in March.

### b) Log transform

We have that the seasonal variations in our data that pushes the range upwards, here is a histogram of the sales.
```{r}
hist(fancy)
```
A logarithm transform would allow us to make the data more normal and therefore we could use the linear model, the transformed histogram is shown below.

```{r}
hist(log(fancy))
```

### c) Log regression

Below are the results of using the log transfromed fancy with a seasonal fit.
```{r}
fancy.ln <- log(fancy)

# Surfing festival dummy
fest <- rep(0, length(fancy))
fest[seq_along(fest)%%12 == 3] <- 1
# The festival started in 1988, our data goes back to 1987
fest[3] <- 0 
# Save it as a timeseries
fest <- ts(fest, start = c(1987,1), freq = 12)

fancy.df <- data.frame(fancy.ln, fest)
lm.ln <- tslm(fancy.ln ~ trend + season + fest, data = fancy.df)
export_summs(summ(lm.ln), output='html', 
             statistics = c("N" = "nobs", 
                            "R squared" = "r.squared", 
                            "P value" = "p.value",
                            "AIC" = "AIC"),
             model.names = c('Log Fancy Sales'))
```

### d) Log model residuals

```{r}
plot(lm.ln$residuals,
     main = 'Residuals of Log Model for fancy sales',
     ylab = 'Residuals',
     type = 'p')
```

I don't see anything particularly interesting with the residuals against time.

```{r}
data.frame(resid = lm.ln$residuals, fit = lm.ln$fitted.values) %>% 
  ggplot(aes(x=fit, y=resid)) + geom_point() + 
  labs(title = 'Fitted Values vs Residuals for Log Fancy Regression',
       y = 'Residuals',
       x = 'Fitted Values')
  
```

There is nothing too out of the ordinary with the fitted values either. 

### e) Box plots
```{r}
boxplot(res ~ month, data = data.frame(res = lm.ln$residuals, month = rep(1:12,7)),
        main = 'Boxplot of Resdiuals by Months')
```

From the boxplots we can see that months 8 (August), 9 (September), and 10 (October) can have a high variance, in addition to months 5 (April) and 3 (March).

### f) Coefficients

Because we have a log transformed dependent variable, we would have to interpret the exact effect in terms of log effects. Otherwise, we can look at the sign and magnitude of our effects. The coefficients of each season, or month variable, tell in which direction sales move during that season. All of these variables are positive which matches the graphs since the tend is increasing. The trend and the festival coefficients are also important in forecasting and determining the pattern of sales. 

### g) Breusch-Godfrey test
```{r}
lmtest::dwtest(lm.ln, alt="two.sided")
```

The DW statistic is significant which indicates that our model is still suffering from autocorrelation which violates the linear model assumptions the are required to fit a model. 

### h) Predictions
```{r}
# new festival dates
new_fes = rep(0, 36)
new_fes[seq_along(new_fes)%%12 == 3] <- 1

forecast <- forecast(lm.ln, newdata=data.frame(fest = new_fes))

knitr::kable(forecast) %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", 
                            full_width = F)
  
```

### i) Transfromed predictions
```{r}
forecast$mean <- exp(forecast$mean)
forecast$upper <- exp(forecast$upper)
forecast$lower <- exp(forecast$lower)
forecast$x <- exp(forecast$x)

knitr::kable(forecast) %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", 
                            full_width = F)

```
```{r}
plot(forecast)
```

### j) Model improvements

I think we could have used a seasonal multiplicative model to forecast the future sales, since there is clearly an increasing seasonal trend. 

## 5.6 Gasoline
### a) Harmonic regression

Lets first plot the data of the entire dataset.
```{r fig.cap = 'Gasoline measured in millions of gallons a day'}
autoplot(gasoline) +
  labs(title = 'Supplies of US finished motor gasoline product',
       y = 'Gasoline')
```

Lets try a fourier series with harmonic k = 1 and k = 5 first. 
```{r}
# We can only use up to 2004 for our training data
gas.2004 <- window(gasoline, end = 2005)

# First order
fit.1 <- tslm(gas.2004 ~ trend + fourier(gas.2004, K = 1))
# Fifth order
fit.5 <- tslm(gas.2004 ~ trend + fourier(gas.2004, K = 5))

autoplot(gas.2004) +
      autolayer(fit.1$fitted.values,
                series = 'K = 1',
                size = 2) +
      autolayer(fit.5$fitted.values,
                series = 'K = 5',
                size = 1.2) + 
      ggtitle('Fourier Transform K=1 and K=5') +
      ylab("Gasoline")

```

We can see that a harmonic with K=5 tracks the information better than a K=1 harmonic, which is more rigid being that it is only order one. Now we can see the improvement that K=10, 15, and 20 add to our model. 

```{r}
fit.10 <- tslm(gas.2004 ~ trend + fourier(gas.2004, K = 10))
fit.15 <- tslm(gas.2004 ~ trend + fourier(gas.2004, K = 15))
fit.20 <- tslm(gas.2004 ~ trend + fourier(gas.2004, K = 20))

autoplot(gas.2004) +
      autolayer(fit.5$fitted.values,
                series = 'K = 5',
                size = 2.5) + 
        autolayer(fit.10$fitted.values,
                series = 'K = 10',
                size = 2) + 
        autolayer(fit.15$fitted.values,
                series = 'K = 15',
                size = 1) + 
        autolayer(fit.20$fitted.values,
                series = 'K = 20',
                size = 0.75) + 
      ggtitle('Fourier Transforms K=5 to K = 20') +
      ylab("Gasoline")

```
We can see from the overlays that the next iterations produce a better a fit although the amplitude seems to get smaller but the variance is captured better.

### b) Model selection
```{r}

cvs <- as.data.frame(rbind(CV(fit.1), CV(fit.5), CV(fit.10),
                           CV(fit.15), CV(fit.20)))

rownames(cvs) <- c('K = 1', 'K = 5', 'K = 10',
                'K = 15', 'K = 20')

cvs %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", 
                            full_width = F)
```

From these we can see that we should choose the K = 10 model, the K values above 10 overfit the training data.

###c) Residuals

Below are the residuals graphs
```{r}
checkresiduals(fit.10)
```

### d, e) Forecast and Plot
```{r}
forecast <- forecast(fit.10,
                     newdata=data.frame(fourier(gas.2004, K = 10, h = 52)))

autoplot(window(gasoline, start = 2004, end = 2006))+
  autolayer(forecast) + 
  labs(title = 'Forecasts of 2006',
       y = 'Gasoline')
```

We can see that our forecasting interval does a pretty good job of predicting the next year's actual value. Below is a graph without the interval.
```{r}
autoplot(window(gasoline, start = 2004, end = 2006))+
  autolayer(forecast$mean, color = 'blue') + 
  labs(title = 'Mean Gasoline Forecasts',
       y = 'Gasoline')
```

When we remove the interval we see that the model does not predict the large drop in mid 2005, this might be a seasonal event or an unforeseen one. But this drop is not too surprising, it is within the 95% interval.

## 5.7
### a) Data exploration
```{r fig.cap = 'Water level in Feet'}
data("LakeHuron")
autoplot(LakeHuron) + 
  labs(title = 'Water Level of Lake Huron over Time',
       y = 'Water Level')
```

We don't see a very obvious trend in the data, there might be some seasonality or cyclicality but its hard to tell and it appears that the long term average level of the lake is rather constant.

### b) Piecewise and linear model

```{r}
# Regular linear model
lm.water <- tslm(huron ~ trend)
export_summs(summ(lm.water), output='html', 
             statistics = c("N" = "nobs", 
                            "R squared" = "r.squared", 
                            "P value" = "p.value",
                            "AIC" = "AIC"),
             model.names = c('Water Level'))
```

```{r}
# Piecewise
t <- time(huron)
t.break <- 1915
t.piece <- ts(pmax(0,t-t.break), start=1875)
pw.huron <- tslm(huron ~ t + t.piece)
export_summs(summ(pw.huron), output='html', 
             statistics = c("N" = "nobs", 
                            "R squared" = "r.squared", 
                            "P value" = "p.value",
                            "AIC" = "AIC"),
             model.names = c('Water Level'))
```

We can see that, due to the lower AIC, the piecewise function appears to be a better fit for the data than the linear model.

### c) Forecasts
```{r}
# 1980 is 8 years forward
# First lets fit the linear trend
forecast <- forecast(lm.water, h=8)

# Then the piecewise
t.new <- t[length(t)] + seq(8)
t.piece.new <- t.piece[length(t.piece)]+seq(8)

forecast.pw <- forecast(pw.huron,newdata =  data.frame('t' = t.new, 't.piece' = t.piece.new))

# Plotting the linear
autoplot(huron) +
  autolayer(forecast) + 
  autolayer(lm.water$fitted.values, color = 'blue') +
  labs(title = 'Water Level of Lake Huron over Time',
       y = 'Water Level')
  
```

```{r}
autoplot(huron) +
  autolayer(forecast.pw) +
  autolayer(pw.huron$fitted.values, color = 'blue') +
  labs(title = 'Water Level of Lake Huron over Time',
       y = 'Water Level')
```

We can see that the piecewise model captures the initial drop that we see in the series, where as the linear model fails to do so.

# Chapter 6 Exercises
## 6.1 Moving Averages

Centered moving averages can be smoothed by another moving average. This creates a double moving average. In the case of a 3x5 moving average, this signifies a 3 period moving average of a 5 period moving average. 

This means we have three periods, weight 1/3 each of our observations averaged by 5. We have these three parts that are simplified below:

$$
(\frac{X_1+X_2+X_3+X_4+X_5}{5}*\frac{1}{3}) + \frac{X_2+X_3+X_4+X_5+X_6}{5}*\frac{1}{3} + \frac{X_3+X_4+X_5+X_6+X_7}{5}*\frac{1}{3} \\
X_1*\frac{1}{15} + X_2*\frac{2}{15} + X_3*\frac{3}{15}+X_4*\frac{3}{15}+X_5\frac{3}{15}+X_6\frac{2}{15}+X_7\frac{1}{15}
$$

Which are equivalent to the stated weights for the 7 period MA.

## 6.2 Plastics 
### a) Data exploration
```{r fig.cap = 'Units are in thousands'}
autoplot(plastics) + 
  labs(title = 'Monthly Sales',
       y = 'Plastic Sales',
       x = 'Month')
```

We can see a seasonal fluctuation with an upward trend for the monthly sales. From eyeballing the plot, I would say that the seasonal trend is additive. 

### b, c) Multiplicative decomposition
```{r}
plastics %>% decompose(type="multiplicative") %>%
  autoplot() + 
  labs(x = "Month") +
  ggtitle("Monthly Product Sales Decomposition")
```

We can see from the graph that indeed there is a seasonal pattern in the data in addition to a general upward trend. 

### d) Seasonally adjusted data
```{r}
autoplot(plastics) + 
  labs(title = "Monthly Sale Predictions", 
       y="Plastic Sales", x = 'Month') + 
  autolayer((plastics/decompose(plastics, type="multiplicative")$seasonal),
            series = 'Decomposed')
```

### e) Outlier on decomposition
```{r}
# For reproducability
set.seed(42)
outlier <- sample(1:length(plastics), 1)
plastics2 <- plastics
plastics2[outlier] <- plastics[outlier] + 500

autoplot(plastics2) +
  labs(title = "Monthly Sales, Observation 19 Outlier",
       y="Plastic Sales", x = 'Month') + 
  autolayer((plastics2/decompose(plastics2, type="multiplicative")$seasonal),
            series = 'Decomposed')
```

We can see that the decomposition picked up on the outlier and included it in the backtrack forecasting. 

```{r}
# For reproducability
plastics3 <- plastics
plastics3[(length(plastics)-1)] <- plastics[(length(plastics)-1)] + 500

autoplot(plastics3) +
  labs(title = "Monthly Sales, End Series Outlier",
       y="Plastic Sales", x = 'Month') + 
  autolayer((plastics3/decompose(plastics3, type="multiplicative")$seasonal),
            series = 'Decomposed')
```

We can see that an outlier near the end can also be handled pretty well by the decomposition. 

## 6.3 Retail series
```{r}
temp = tempfile(fileext = ".xlsx")
url <- 'https://otexts.com/fpp2/extrafiles/retail.xlsx'
download.file(url, destfile=temp, mode='wb')
retail <- readxl::read_excel(temp, skip = 1)
retail.ts <- ts(retail[,17], frequency=12, start=c(1982,4))

autoplot(retail.ts, main="Monthly Austrailian Retail Sales", ylab="Sales", xlab="Year")
```

```{r}
library(seasonal)
retail.x11 <- seas(retail.ts, x11="")

autoplot(retail.x11, 
         main="Monthly Austrailian Retail Sales X11 Decomposition", 
         xlab="Year")
```

The last series in the decomposition graph reveals outliers around 1987, 1993/4, 2003, and others in the 2010s. We were previously unable to see these in the original graphs.

## 6.4 Interpretation of Graphs
### a) Results 

The graph shows the number of persons in the civilian labor force in Austrialia each month from February 1978 to August 1995. There is a strong positive trend in the graph. The decomposition shows the growth in the labor is cyclical with seasonality. Looking at the last series, there are some major outliers around 1992/3. This could be a one time event in Australia that affected the labor force, maybe something like a census accounting change. The second chart measures the degree of seasonality and shows larges for July and decrease in March. 

### b) Recession

The 1991/2 recession is visible from the irregular series at the end, we can see major decreases around those years in the labor force for the months in that year.

## 6.5 Canadian Gas
### a) Different plots
```{r}
data(cangas)

autoplot(cangas, 
         main = 'Canadian Oil Production',
         ylab = 'Gas Production in Billions of Cubic Metres',
         xlab = 'Year')
```

```{r}
ggsubseriesplot(cangas, 
         main = 'Canadian Oil Production Sub Series',
         ylab = 'Gas Production in Billions of Cubic Metres',
         xlab = 'Month')
```

```{r}
ggseasonplot(cangas,
             main = 'Canadian Oil Production Seasonal Plot',
             ylab = 'Gas Production in Billions of Cubic Metres',
             xlab = 'Month')
```

I think that the change in seasonality over the different years, the fact that it is less pronounced, might be due to two things. One is technological change that allows more steady production of oil throughout the year.  The second is probably new oil reserves being found throughout time.

### b) STL decomposition
```{r}
stl(cangas, 
    t.window=13, 
    s.window="periodic", 
    robust=TRUE) %>%
  autoplot(main = 'STL Decomposition', xlab = 'Years')
```

### c) Compare decompositions

The results of the decompositions are somewhat similar. The remainder plot shows us all the outliers that between 1980 and 1990 that might've been affecting prvious breakdowns. These outliers are smaller later on which explains the decrease in seasonality that was discussed in previous sections.

## 6.6 Canadian Clay Brick Production
### a) STL decomposition
```{r}
stl(bricksq, 
    t.window=20, 
    s.window="periodic", 
    robust=TRUE) %>%
  autoplot()
  
```

### b) Seasonally adjusted plots
```{r}
autoplot(bricksq, 
         main="Clay Production", ylab="Units", xlab="Year") + 
  autolayer((bricksq-decompose(bricksq, type="additive")$seasonal),
            series = 'Decomposed')
```

### c) Naïve method
```{r}
autoplot(bricksq, 
         main="Clay Production", ylab="Units", xlab="Year") +
  autolayer(naive((bricksq-decompose(bricksq, type="additive")$seasonal), 
                  h=30), series="Naïve")
```

### d) Reseasonalize 
```{r}
brick.rs <- stlf(bricksq)
autoplot(brick.rs)
```

### e) Residuals
```{r}
checkresiduals(brick.rs)
```

It looks like the residuals are for the most part normally distributed.

### f) Robust STL
```{r}
brick.rs <- stlf(bricksq, robust=TRUE)
autoplot(brick.rs)
```
```{r}
checkresiduals(brick.rs)
```

It doesn't seem to have made much of a difference. In fact, there are more autocorrelation problems with the robust version.

### g) Naive comparison
```{r}
Brick.train <- subset(bricksq, end=length(bricksq) - 9)
Brick.test <- subset(bricksq, start=length(bricksq) - 8)

brick <- snaive(Brick.train)
brick.stlf <- stlf(Brick.train, robust=TRUE)

autoplot(brick)
```
```{r}
autoplot(brick.stlf)
```
It looks like the forecasts with STLF are less variable and petter predict production.

### 6.7 Writing
```{r}
autoplot(writing,
         main = 'Industry sales for printing and writing paper',
         ylab = 'Thousands of French francs',
         xlab = 'Year')
```

It looks like there is drift in the data.

```{r}
stlf(writing, method='rwdrift', robust=TRUE, lambda = BoxCox.lambda(writing)) %>% 
autoplot()
```

### 6.8 Fancy revisited
```{r}
autoplot(fancy,
         main = 'Souvenir Shop Sales', ylab = 'Sales',
         xlab = 'Year')
```

There is clearly a positive trend so lets once again use the drift adjustment.

```{r}
stlf(fancy, method='rwdrift', robust=TRUE, lambda = BoxCox.lambda(writing)) %>% 
autoplot()
```




